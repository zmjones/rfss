% Random Forests for the Social Sciences
% Zachary Jones and Fridolin Linder[^contact]

[^contact]: Zachary M. Jones is a Ph.D. student in political science at Pennsylvania State University ([zmj@zmjones.com](mailto:zmj@zmjones.com)). Fridolin Linder is a Ph.D. student in political science at Pennsylvania State University ([fridolin.linder@gmail.com](mailto:fridolin.linder@gmail.com)).

\begin{abstract}
We introduce random forests, a nonparametric supervised learning algorithm that allows researchers to nonparametrically describe the relationship between an outcome that is continuous, categorical, or censored (survival). Random forests detect interaction and nonlinearity without prespecification, have low generalization error, do not overfit, and can be used with many correlated predictors. Importantly, they can be interpreted in a substantively relevant way via measures of marginal variable importance and the partial dependence algorithm. We provide intuition as well as technical detail about how random forests work, in theory and in practice, as well as an empirical example from the literature on comparative politics. We also provide to facilitate the substantive interpretation of random forests and guidance on when random forests may be useful.
\end{abstract}

## Introduction

Political scientists have, in recent years, begun to utilize more flexible algorithmic methods for inferring substantive relationships from data [@beck1998beyond; @beck2000improving; @hainmueller2013kernel; @hill2014empirical]. These methods can often outperform more commonly used regression methods at predicting data not used to fit the model, which is useful for policymakers and serves as a useful check of the explanatory power of our theories [@hill2014empirical]. Many of these methods are commonly thought of as "black box," that is, they predict well, but do not permit substantive interpretation [@breiman2001statistical]. We show that this is *not* the case with a broadly applicable, powerful, and underappreciated method (in political science): random forests [@breiman2001random]. Random forests are especially useful to political scientists because of their ability to approximate arbitrary functional forms, be used with continuous, discrete, and censored (survival) outcomes, and because they permit substantive interpretation via permutation importance measures and the partial dependence algorithm. We provide an introduction to the theory and use of random forests, a substantive example drawn from the literature on comparative politics, and provide software to make substantive inference based on random forests easy.

We think that random forests would be useful in political science when relevant theory says little about the functional form of the relationship of interest, when the magnitude and degree of nonlinearity and interaction is unknown, when the number of possibly relevant predictors is large, and when prediction is important. Random forests can approximate many nonmonotone, nonlinear functional forms. Interactions and nonlinearities are identified by random forests without prespecification, which decreases prediction error and allows researchers to study the relationships discovered by the algorithm. Random forests allow the inclusion of more predictors than observations. Though this situation is not common in political science (though see the literature on behavioral genetics, wherein this problem does occur), related issues such as a highly correlated predictors are not an issue for random forests. Prediction is important for theory evaluation and for policymakers, and random forests' predictive performance relative to common parametric regression methods and other nonparametric methods is strong.

Random forests are composed of classification and regression trees (CART). A CART is created by recursive partitioning, which is a method for finding homogeneous (in the outcome variable) subsets of the data using the predictors [@breiman1984classification]. Different definitions of homogeneity allow CARTs to be used with different types of outcomes. The fitted values of CARTs have low bias but high variance. Random forests where developed to solve this problem via two mechanisms. First, growing multiple decision trees with bootstrap samples of the data, and then averaging over the predictions made by each tree: a procedure known as "bagging". And second, randomly selecting a subset of the predictors at each  possible split in each CART.

Since the logic of CARTs is central to the understanding of random forests we explain their logic in detail in the first section. We then proceed to present random forests, followed by a discussion of methods for extracting substantively relevant information from them, and an empirical example on....


## Classification and Regression Trees

Classification and regression trees (CART) are a regression method that relies on repeated partitioning of the data to estimate the conditional distribution of a response given a set of predictors. Let the outcome of interest be a vector of observations $\textbf{y} = (y_1,y_2,...,y_n)$ and the set of explanatory variables or predictors a matrix $\textbf{X} = (\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_k)$, where $\textbf{x}_i = (x_{i1},x_{i2},...,x_{in})$ for $i \in \{1,2,...,k\}$. The goal of the algorithm is, to partition $\textbf{y}$ conditional on the values of $\textbf{X}$ in such a way that the resulting subgroups of $\textbf{y}$ are as homogeneous as possible.  

The algorithm works by considering every unique value in each predictor as a candidate for a binary split, and calculating the homogeneity of the subgroups of the outcome variable that would result by grouping observations that fall on either side of this value. Consider the (artificial) example in Figure \ref{fig:cart_visu}. $\textbf{y}$ is the vote choice of $n = 40$ subjects (18 republicans and 22 democrats), $\textbf{x}_1$ is the ideology of the voter and $\textbf{x}_2$ is the age. 

The goal of the algorithm is to find homogeneous partitions of $\textbf{y}$ given the predictors. The algorithm starts at the upper right panel of Figure \ref{fig:cart_visu}, the complete data which is the first node of the tree. We could classify all cases as democrats yielding a misclassification rate of $18/40 = 0.45$. But it is obvious that there is some relationship between ideology and vote choice (the D's are mostly on the right side and the R's mostly on the left side), so we could do better in terms of classification error using this information. Formally the algorithm searches through all unique values of both predictors and calculates the number of cases that would be misclassified, if a split would be made at that value and all cases on the left and right of $c$ are classified according to majority rule. The upper right panel displays this step for one value of ideology (which also turns out to be the best possible split). In the tree in the lower left panel of Figure \ref{fig:cart_visu} the split is indicated by the two branches growing out of the first node. The variable name in the node indicates that the split was made on ideology. To the left of $c = 3.31$ most of the subjects voted republican and on the right most voted democrat. Therefore we classify all cases that have with ideology lower then $3.21$ as republican and all cases with ideology greater than $3.21$ as democrats (indicated by the shaded areas in the scatterplots). Now only eight cases are misclassified, yielding an error rate of $8/40 = 0.2$. 

![Visualization of a recursive partitioning algorithm for classification. The upper left panel displays the original data. The two panels on the right display the partitions of the original data after the first and the second split respectively. The lower left panel displays the corresponding decision tree. The blue and red shaded areas in the right panels indicate the value for the fitted value of the terminal node. The shading of the area visualizes classification as republican (red) or democrat (blue) by majority rule. The red colored letters indicate incorrect classifications under this rule. \label{fig:cart_visu}](figures/cart_visu.png)

The algorithm then proceeds to look for the next better split within the two new partitions (left and right of $c_{x_1} = 3.21$. It turns out that for the right side there is no good split to make to decrease the misclassification rate (we talk about the stopping criteria to make this decision later). This is shown in the tree as a so called terminal node on the right branch of the ideology split. The plot in the terminal node displays the distribution of cases in this partition of the data. 

However, age still contains information to improve the partitioning. At the second node (i.e. all data that falls left of the first split), when splitting the data in all subjects older and younger then 51 years, we can obtain a completely homogeneous partition where all subjects voted republican. Additionally those subjects older then $51$ and with an ideology value lower than $3.21$ are now classified as democrats. Note that the four democratic cases in this region of the data, that where misclassified before, are now correctly classified, in turn the three republican in the upper right partition are now misclassified. The classification error has therefore again been reduced from $8/40$ to $6/40$.

We now proceed to extend the logic of CARTs from this very simple example of binary classification with two continuous predictors to dependent variables with other levels of measurement.

When extending the algorithm to other dependent variables we have to explicitly think about loss functions. We used the concept of a loss function in the illustration above already. We calculated the classification error when just using the modal category of the outcome variable and argued that further splits of the data are justified because they decrease this error. More formally let $\textbf{y}^m = (y^m_1,...,y^m_{n^m})$ be the data at the current node $m$, $\textbf{x}^m_s$ the predictor that is considered to be used for a split with unique values $C = \{x^m_i\}_{i\in \{1,...,n^m\}}$ and $c \in C$ the value considered for a split. Then the data left and right of the split is $\textbf{y}^m_l = \{y^m_i:x^m_i \leq c, i = 1,...,n^m\}$ and $\textbf{y}^m_r = \{y^m_i:x^m_i > c, i = 1,...,n^m\}$. The gain from a split at node $m$ in predictor $s$ at value $c$ is defined as:

$$\Delta_{m,s,c} = L(\textbf{y}^m) - \left[ \frac{n^m_l}{n^m} L(\textbf{y}^m_l) +  \frac{n^m_r}{n^m} L(\textbf{y}^m_r)\right]$$.

Where $n^m_l$ and $n^m_r$ are the number of cases that fall to the right and to the left of the split, and $L(.)$ is the loss function. 

In the example above we made the intuitive choice to use the number of cases incorrectly classified when assigning the mode as the fitted value, divided by the number of cases in the node, as the loss function. This proportion can also be interpreted as the impurity of the data in the node, to return to our goal stated at the beginning: to partition the data in a way that produces homogeneous subgroups. Therefore it is intuitive to use the amount of impurity as a measure of loss. It is now obvious how the algorithm can be used for outcomes wit more than two values (i.e. ordinal and nominal as well as continuous outcomes). By choosing a loss function that is appropriate to measure the impurity of a variable at a certain level of measurement, the algorithm can be easily extended to those outcomes. 

For categorical outcomes, denote the set of unique categories of $\textbf{y}^m$ as $D = \{y^m_i\}_{i\in\{1,...,n^m\}}$[^ordered]. In order to asses the number of misclassified cases we first calculate the proportion of cases pertaining to class each class $d \in D$ from:

$$p^m(d) = \frac{1}{n^m}\sum_{i=1}^{n^m} \mathbb{I}(y^m_i = d), d \in D$$.

Where $\mathbb{I}(.)$ denotes the indicator function. $p^m(d)$ is maximized to find the best assignment for the cases in the node: $\hat{y} = \operatorname*{arg\,max}_d p^m(d)$ (this is what we referred to as majority vote above). Then the loss function can be applied to obtain the impurity of the node. The familiar misclassification loss[^loss] is obtained from:

$$\frac{1}{n^m} \sum_{i = 1}^{n^m} \mathbb{I}(y^m_i \neq \hat{y}) = 1 - p^m(\hat{y})$$.

[^loss]: The other two loss functions that are most often used are the Gini loss $\sum_{d \in D} p^m(d)[1-p^m(d)]$, and the cross-entropy or deviance $-\sum_{d \in D} p^m(d)log[p^m(d)]$.

In the continuous case, the fitted value in a node is not calculated by majority vote, but, similar to regression, by assigning the mean of the distribution in that node to each observation. To measure the impurity of the node usually the mean squared error (MSE) is used: 

$$\sum_{i=1}^{n^m} (y^m_i - \bar{y}^m)^2$$.

With survival data, a log-rank statistic is often used. Let $T$ be the time of an event and $C$ the censoring time, then the data consist of $N$ observations $(y_i, \delta_i, \textbf{x}_i)$ where $y_i=\text{min}(T,C)$ and $\delta_i=\mathbb{I}(T \geq C)$. A  given node $h$ in a tree contains $M$ observations with survival times $t_1 < t_2 < \ldots < t_m$. A proposed split results in two daughter nodes $(j=1,2)$ containing a certain amount of cases. Let the $d_{i,j}$ denote the number of deaths and $y_{i \; j}$ the number of observations at risk at time $t_i$ in daughter node $j$ respectively. Then the survival difference resulting from a split at $c$ in predictor $X$ is measured by the log-rank score:

$$
L(x,c) = \frac{\sum_{i=1}^M d_{i1} - y_{i1}\frac{d_i}{y_i}}{\sqrt{\sum_{i=1}^M \frac{y_{i1}}{y_i}(1-\frac{y_{i1}}{y_i})(\frac{y_i-d_i}{y_i-1})d_i}}
$$

At each terminal node the cumulative hazard function (CHF), denoted $H$, is estimated. Each observation in terminal node $h$ is assigned the same CHF, where:

$$\widehat{H}(t|h) = \sum_{t_{lh} \leq t} \frac{d_{lh}}{y_{lh}}$$ 

The choice of a loss function is in part determined by the type of outcome variable (using the misclassification loss for a continuous outcome would make no sense). Which loss funcion to choose within the three categories that we discussed here is a different question [DISCUSS THAT: gini entropy or misclassification doesn't matter but you can get creative, see Berk. dk if we want to include that]

After a loss function is chosen, the algorithm proceeds as described in our example. At each node $m$, $\Delta_{m,s,c}$ is calculated for all variables and all possible splits in the variables. The variable-split combination that produces the highest $\Delta$ is selected and the process is repeated for the data in the resulting daughter nodes $\textbf{y}^m_l)$ and $\textbf{y}^m_r)$ until a stopping criteria is met. The stopping criteria may be that the tree has reached sufficient depth, that the number of observations that fall into the daughter nodes is too small, or the distribution of $\textbf{y}^m)$ is sufficiently homogeneous. These stopping criteria are arbitrary and should be understood as tuning parameters. 

CART has two main problems: fitted values have high variance, and there is a substantial risk of overfitting. Fitted values, the value of the response predicted by the algorithm, can be unstable, producing different classifications when changes to the data used to fit the model are made [DISCUSS IN A BIT MORE DETAIL]

Another issue is the inclusion of categorical variables with many values. With structured data such as is common in political science, it may be interesting to ask how much predictive power is lost by permuting unit labels, and how different predictors interact with unit labels. As may be clear from the splitting proccess described above, the combinatorics of an exhaustive search over possible splits in a categorical variable quickly becomes computationally prohibitive. A categorical variable with $n$ categories can be split in $2^{n-1}-1$ ways. An alternative would be to create a binary variable for each category, however, it is beyond the scope of this paper to analyze the behavior of the summarization of block permuting a set of binary variables. It is likely that different strategies for modeling unit specific variation must be developed in this situation.

## Random Forests

@breiman2001random extended the logic of bagging to predictors, resulting in random forests. Instead of each tree relying on a bootstrapped sample of observations on all variables, only a random subset of the predictors are used for each splitting node in a tree: increasing the diversity of splits across trees: allowing weaker predictors to have an opportunity to influence the models' predictions. This results in a further decrease in the variance of the fitted values (beyond bagging observations) and allows the use of large numbers of potentially relevant predictors (many more predictors than observations in some cases). A particular observation can fall in the terminal nodes of many trees in the forest, each of which, potentially, can give a different prediction. Typically the OOB data, that is, data that was *not* drawn in the bootstrap sample used to fit a particular tree, is used to make each tree's prediction. This results in a weighted average:

$$f(\textbf{X})= \frac{1}{M} \sum_{m=1}^M f_m(\textbf{X}_{\text{oob}})$$

where $M$ is the total number of trees in the forest, and $f_m(\cdot)$ is the $m$'th tree, and $\textbf{X}_m$ is the OOB data for that tree.

With survival data the averaging procedure is not as simple. For the OOB ensemble estimate $\widehat{H}^*$ of the CHF of an observation $i$, $\widehat{H}$ is averaged over all trees that where grown without the observation. The prediction error rate is calculates using Harrel's concordance index. The basic intuition behind this index is to check for each pair of observations if the predicted risk is higher for the observation that failed first. The predicted risk for observation $i$ with covariate vector $x_i$ is obtained from:

$$R = \sum_{k=1}^n \widehat{H}^*(t_k|\textbf{X}_i)$$

If this is the case, the prediction counts as correctly classified.


Lastly, there is the issue of bias in the splitting procedure, meaning that some CART algorithms tend to select some sorts of variables more than they should. Variables with many categories and large amounts of missingness have been shown to generate this bias, which results in biased permutation importance measures [@hothorn2006unbiased; @strobl2007bias]. The key difference in the splitting procedure is the separation of variable selection (in which variable the optimal split will be found) and the split search procedure. For substantive work we recommend that researchers use splitting criteria with good frequentist properties, though this must be counterbalanced with computational limitations. [UNCLEAR]



### Substantive Interpretation

Since, with random forests both predictors and observations are being sampled, no particular tree will give great insight into the model's overall prediction for each observation. There are, however, several ways to extract inferences from random forests. The most simple is partial dependence [@hastie2009elements]. The partial dependence algorithm works as follows:

 1. Let $X_i$ be the predictor of interest, $\textbf{X}_{-i}$ be the other predictors, $y$ be the outcome, and $f(\textbf{X})$ the fitted forest.
 2. If $X_i$ has $k$ unique values $V = (v_1,v_2,...,v_k)$, create $k$ new datasets, where $X_i = v_k$ for $\forall \: v$ and all $\textbf{X}_{-i}$ are the same as in the original dataset.
 3. Drop each of the new datasets, $X_i^{v_k}, \textbf{X}_{-i}$ down the fitted forest, resulting
 in a predicted value for each observation in all $k$ datasets, $\hat{y}_{X_i=v}$.
 4. Summarize the predictions in each of the $k$ datasets (for example by taking the mean).
 5. Visualize the relationship by plotting $V$ against $\hat{y}_{X_i=v}$.

With slight modification, this method can also be used to visualize any joint relationships (i.e. interactions) the algorithm may have found. To do this create a dataset for each of the possible combinations of unique values of the explanatory variables of interest, predict the outcome in each of these observations, and then find the mean or modal prediction for each of these unique value combinations. For computational reasons we do not always use every unique value when an explanatory variable takes on more an arbitrary number of unique values. In this paper we use a random sample of 24 unique values that $X_i$ takes on.[^extrapolation] This logic can be generalized to joint relationships of an arbitrary dimension, but we limit ourselves here to pairwise partial dependence. The interpretation of partial dependence: the average predicted value for a particular value of an explanatory variable averaged within the joint values of the other predictors.

[^extrapolation]: It is also possible to use an evenly spaced grid, however, this may result in extrapolation. Both of these options are implemented in our R package [edarf](https://github.com/zmjones/edarf/).

Another approach to extracting inferences from random forests relies on permutation tests for variable importance. Rather than attempting to characterize the partial dependence of one or more predictors on the response, the goal is instead to describe how the model's ability to predict $y$ depends on a particular predictor. If a particular column of $\textbf{X}$, say $X_i$, is unrelated to $y$, then randomly permuting $X_i$ within $\textbf{X}$ should not meaningfully decrease the model's ability to predict $y$. However, if $X_i$ is strongly related to $y$, then permuting its values will produce a systematic decrease in the model's ability to predict $y$, and the stronger the relationship between $X_i$ and $y$, the larger this decrease. Averaging the amount of change in the fitted values from permuting $X_i$ across all the trees in the forest gives the marginal permutation importance of a predictor.[^marginal] Formally, for classification, the importance of explanatory variable $X_i$ in tree $t$ is:

$$VI^{(t)}(X_i) = \frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_i^{(t)})}{|\bar{\mathcal{B}}^{(t)}|} -
\frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_{i \pi j}^{(t)})}{|\bar{\mathcal{B}}^{(t)}|}
$$

where $\bar{\mathcal{B}}$ is the out-of-bag data for tree $t$, $X_i$ is a particular explanatory variable, $\hat{y}_i^{(t)}$ is the fitted value for observation $i$ in tree $t$, $\hat{y}_{i \pi j}^{(t)}$ is the predicted class after permuting $X_i$, and $|\bar{\mathcal{B}}^{(t)}|$ is the number of observations in the sample used for fitting tree $t$. The importance of variable $X_i$ in tree $t$ is averaged across all trees to give the permutation importance for the forest [@breiman2001random; @strobl2008conditional]. This can be thought of as testing the null hypothesis of independence between $X_i$ and $y$ as well as all other explanatory variables $\textbf{X}_{-i}$. That is, $\mathbb{P}(y, X, \textbf{X}_{-i}) = \mathbb{P}(X_i)\mathbb{P}(y, \textbf{X}_{-i})$. For regression, the permutation is defined similarly, by the average increase in the mean squared error across trees that results from permuting $X_i$.

[^marginal]: This measure is not truly marginal since the importance of a variable within a particular tree is conditional on all previous splits in the tree. It is possible to conduct a conditional permutation test which permutes $X_i$ with variables related to $X_i$ "held constant," reducing the possibility that a variable is deemed important when it is actually spurious [@strobl2008conditional]. However, this procedure is prohibitively costly in terms of computational resources.

### Dependent Data

As previously mentioned, these methods are not designed for use with dependent data, such as is common in political science. Not modeling the dependence structure may decrease predictive performance on new data and mislead us about the importance of variables strongly related to different features of the unmodeled dependence structure. In its basic implementation the estimated regression function is the result of complete pooling of the data. There are several ways this dependence can be modeled. One way is to include a categorical variable with unit indicators as an explanatory variable. Then, this explanatory variable has a chance of being included in the set of variables that the algorithm may select for splitting at a particular node. This is computationally intensive and not always possible. Alternatively, a random effects approach could be used: the outcome of interest is treated as a function of an unknown regression function, which is estimated using random forests and completely pools the data, and set of unit random effects for which we estimate the variance, and idiosyncratic error which is assumed uncorrelated with the random effects [@hajjem2014mixed; @hajjem2011mixed]. Alternative approaches include sampling units [@adler2011ensemble] or sampling units and then an observation from within that unit [@adler2011ensemble]. Alternatively, the analyst can transform the dependent variable, such as by subtracting the within-unit mean. This, however, invalidates the use of the fitted random forest for forecasting. In the following applications we discuss what approach we use and leave the development of other approaches to future work.

### Missingness

Missing values are a perennial problem in real world data. Often missingness is ignored, which, at best, decreases the precision of estimates, and at worst biases them. Multiple imputation is frequently used to provide estimates of the values of missing values: this relies on the assumption that, conditional on the observed covariates, missingness is random [@rubin2004multiple; @honaker2010missing]. When analysis is exploratory or predictive, missingness has a different epistemological status and can be *useful* [@shmueli2010explain]. When the goal is prediction alone, informative missingness improves our ability to predict the outcome, and when doing exploration, understanding how missingness is informative is useful for future work. When predictors with missingness are categorical missingness can simply be recoded as an additional category. With continuous variables this is not possible. Additionally, this is not always optimal however. When using permutation importance to measure the degree to which a model's fit is degraded by permuting said variable recoding in this way conflates missingness that is informative with how informative the observed values are. An alternative would be to create a set of variables that indicate whether missingness in a particular predictor (or set of predictors) is present. This does not fix the problem of partially observed predictors, but this can be handled using multiple imputation or other ways to handle missing values such as surrogate splitting, which, since unbiased estimation of causal effects is not the goal, do not have to satisfy the stringent conditionally missing at random assumption previously mentioned. Frequently data are imputed once, and the data are treated as completely observed. This does however, result in a tendency to underestimate generalization error.

## Example

## Conclusion

# References
