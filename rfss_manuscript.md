% Exploratory Data Analysis using Random Forests[^conf]
% Zachary Jones and Fridolin Linder[^contact]

[^conf]: Prepared for the 73rd annual MPSA conference, April 16-19, 2015.
[^contact]: Zachary M. Jones is a Ph.D. student in political science at Pennsylvania State University ([zmj@zmjones.com](mailto:zmj@zmjones.com)). Fridolin Linder is a Ph.D. student in political science at Pennsylvania State University ([fridolin.linder@gmail.com](mailto:fridolin.linder@gmail.com)) his work is supported by Pennsylvania State University and the National Science Foundation under an IGERT award # DGE-1144860, Big Data Social Science".

\begin{abstract}
The rise of *Big Data* has made machine learning algorithms more visible and relevant for social science researchers, however, they are still widely considered to be "black box" models that are not suited for substantive research but only for prediction. We argue that this need not be the case, and present one method, random forests, with an emphasis on practical application for exploratory analysis and substantive interpretation. Random forests detect interaction and nonlinearity without prespecification, have low generalization error in simulations and in many real-world problems, and can be used with many correlated predictors, even when there are more predictors than observations. Importantly, random forests can be interpreted in a substantively relevant way with variable importance measures, bivariate and multivariate partial dependence, proximity matrices, methods for interaction detection. We provide intuition as well as technical detail about how random forests work, in theory and in practice, as well as empirical examples from the literature on american and comparative politics. We furthermore provide software implementing the methods we discuss to facilitate their use.
\end{abstract}

\clearpage

## Introduction

Technological development has made an abundance of new sources of data available to social scientists. These developments often referred to as *Big Data*, have triggered a debate in political science on how useful these new sources of data and the methods of analysis that come with them are for social scientific inquiry and how compatible they are with classic approaches in political science [@clark2015big]. An important component of these new developments is the prominence of machine learning algorithms that have been used for a long time in computer science and other disciplines, but get more and more visible for social science researchers. These methods, also referred to as data mining, statistical learning or algorithmic models, have initially been developed to maximize predictive performance, and they are most prominently employed for rather a-theoretical tasks. For this reason, they are often considered to be black boxes, that deliver good predictions, but are not very useful for more theory driven work interested in substantive insights [@breiman2001statistical]. 

Although algorithmic methods have spawned some interest in the political methodology literature [@beck1998beyond; @beck2000improving; @hainmueller2013kernel; @imai2013estimating] and have been used for applied work [@grimmer2013text; @hill2014empirical; @dorazio2015theres] in a few instances, they are not very prominent in applied political science research. We argue, that the new developments in the area of machine learning should not be seen as incompatible with traditional avenues of political science research but rather as a useful extension of the data analysis and inference tools that are available to researchers [@monroe2015no]. In order to support this claim, in this paper we demonstrate the usefulness of a well established machine learning algorithm - Random Forests - for applied political science research. In particular, we show how substantive insights for exploratory data analysis (EDA) can be obtained from the Random Forest algorithm.

Although EDA is not very prominent in published work in political science, it can be viewed as a basic building block of every scientific agenda [@tukey1977exploratory; @gelman2004exploratory; @shmueli2010explain]. Especially in settings where only observational data is available, and the observations are generated from a complex processes for which it is difficult to write a generative model, EDA can prove useful. However it can also be useful for experiments and quasi-experiments as well; both for looking for interactions with the treatment(s) as well as studying the predictors and the outcome of interest.

The Random Forest algorithm has receive little attention in political science so far[^exceptions]. In this paper, we demonstrate that Random Forests are useful tools for EDA. They can be easily used with all common forms of outcome variables: continuous, discrete, and censored (survival), and multivariate combinations thereof. Furthermore, they do not require distributional assumptions. They can approximate arbitrary functional forms between explanatory and outcome variables, making it easy to discover complex nonlinear relationships that with many standard methods would be miss if not explicitly specified. These relationships can be visualized and interpreted using partial dependence plots. Furthermore, Random Forests are capable of detecting interactions of any order between predictors, without specifying them in advance. Graphical and maximal subtree methods can be used to extract and substantively interpret these interactions. The importance of variables can be assessed by their impact on the accuracy of predictions, which allows for a quick assessment of the relevance of a predictor for the outcome of interest. Finally, Random Forests can be used to obtain a measure of the similarity of observations in the predictor fspace, which, especially when units are named (e.g. countries), may provide insight into similarities that may not be obvious, and provides further information about the importance of explanatory variables.

In this paper, we provide an accessible introduction to the algorithm and the methods used to extract substantive insights mentioned above. First we give an introduction to classification and regression trees (CART) that are the basic building blocks of the Random Forest. We then describe their combination into an ensemble that is the Random Forest. In the remainder of the paper we describe methods that can be used to extract substantive insights from the fitted forest. We illustrate these methods using two exemplary applications. One is based on data from a recent turnout experiment on ex-felons in Connecticut [@gerber2014can]. The other uses data on the country level predictors for respect for human rights [@hill2014empirical; @fariss2014respect] There are several packages in \texttt{R} to fit Random Forests, however, the methods to extract substantive insight are insufficiently general, do not exploit parallelization when it is possible, are only available in some packages, lack a consistent interface, and lack the ability to generate publication quality visualizations. Therefore, accompanying to this paper, we developed an \texttt{R} package, that allows to easily compute and visualize these methods for EDA with all of the major Random Forest \texttt{R}-packages[^edarf].

[^exceptions]: Although there have been some exceptions @grimmer2015we, @hill2014empirical.
[^edarf]: The package is currently available in its development version at <https://github.com/zmjones/edarf>. The packages supported are [party](http://cran.r-project.org/web/packages/party/index.html) (`cforest`), [randomForest](http://cran.r-project.org/web/packages/randomForest/index.html), and [randomForestSRC](http://cran.r-project.org/web/packages/randomForestSRC/index.html) (`rfsrc`).

## Classification and Regression Trees

Classification and regression trees (CART) are a regression method that relies on repeated partitioning of the data to estimate the conditional distribution of a response given a set of predictors. Let the outcome of interest be a vector of observations $\mathbf{y} = (y_1,\ldots,y_n)^T$ and the set of explanatory variables or predictors a matrix $\mathbf{X} = (\mathbf{x}_1,\ldots,\mathbf{x}_p)$, where $\mathbf{x}_j = (x_{1j},\ldots,x_{nj})^T$ for $j \in \{1,\ldots,p\}$. The goal of the algorithm is, to partition $\mathbf{y}$ conditional on the values of $\mathbf{X}$ in such a way that the resulting subgroups of $\mathbf{y}$ are as homogeneous as possible.

The algorithm works by considering every unique value in each predictor as a candidate for a binary split, and calculating the homogeneity of the subgroups of the outcome variable that would result by grouping observations that fall on either side of this value. Consider the (artificial) example in Figure \ref{fig:cart_visu}. $\mathbf{y}$ is the vote choice of $n = 40$ subjects (18 republicans and 22 democrats), $\mathbf{x}_1$ is the ideology of the voter and $\mathbf{x}_2$ is the age. 

The goal of the algorithm is to find homogeneous partitions of $\mathbf{y}$ given the predictors. The algorithm starts at the upper right panel of Figure \ref{fig:cart_visu}, the complete data is the first node of the tree. We could classify all cases as Democrats yielding a misclassification rate of $18/40 = 0.45$. But it is obvious that there is some relationship between ideology and vote choice (the D's are mostly on the right side and the R's mostly on the left side), so we could do better in terms of classification error using this information. Formally the algorithm searches through all unique values of both predictors and calculates the number of cases that would be misclassified if a split would be made at that value and all cases on the left and right of this split are classified according to the majority rule. The upper right panel displays this step for one value of ideology (which also turns out to be the best possible split). In the tree in the lower left panel of Figure \ref{fig:cart_visu} the split is indicated by the two branches growing out of the first node. The variable name in the node indicates that the split was made on ideology. To the left of an ideology value of $3.31$ most of the subjects voted Republican and on the right most voted Democrat. Therefore we classify all cases on the left and right as Republican and Democrat respectively (indicated by the shaded areas in the scatterplots). Now only 8 cases are misclassified, yielding an error rate of $8/40 = 0.2$.

![Visualization of a recursive partitioning algorithm for classification. The upper left panel displays the original data. The two panels on the right display the partitions of the original data after the first and the second split respectively. The lower left panel displays the corresponding decision tree. The blue and red shaded areas in the right panels indicate the value for the fitted value of the terminal node. The shading of the area visualizes classification as Republican (red) or Democrat (blue) by majority rule. The red colored letters indicate incorrect classifications under this rule. \label{fig:cart_visu}](figures/cart.png)

The algorithm then looks for further splits within the two new partitions (left and right of $c_{x_1} = 3.21$. It turns out that for the right side there is no split that decreases the misclassification rate sufficiently (we talk about the criteria for making stopping decisions later). This is shown in the tree as a so called terminal node on the right branch of the ideology split. The plot in the terminal node displays the distribution of cases in this partition of the data. 

However, age still contains information to improve the partitioning. At the second node (i.e. all data that falls left of the first split), when splitting the data into subjects older or younger then 51 years, we can obtain a completely homogeneous partition where all subjects voted Republican. Additionally those subjects older then $51$ and with an ideology value lower than $3.21$ are now classified as democrats. Note that the four democratic cases in this region of the data, which were misclassified before, are now correctly classified. The three republicans in the upper right partition are now misclassified. The classification error has therefore been reduced from $8/40$ to $6/40$.

We now extend the logic of CART from this very simple example of binary classification with two continuous predictors to other types of outcome variables. When extending the algorithm to other types of outcome variables we have to think about loss functions explicitly. In fact, we used a loss function in the illustration above. We calculated the classification error when just using the modal category of the outcome variable and argued that further splits of the data are justified because they decrease this error. More formally let $\mathbf{y}^{(m)} = (y^{(m)}_1,\ldots, y^{(m)}_{n^{(m)}})$ and $\mathbf{X}^{(m)} = (\mathbf{x}^{(m)}_1,\ldots,\mathbf{x}^{(m)}_p)$ be the data at the current node $m$, $\mathbf{x}^{(m)}_s$ the predictor that is to be used for a split, with unique values $\mathcal{C}^{(m)} = \{x^{(m)}_i\}_{i\in \{1,\ldots,n^{(m)}\}}$ and $c \in \mathcal{C}^{(m)}$ the value considered for a split. Then the data in the daughter nodes resulting from a split in c are $\mathbf{y}^{(m_l)}$ and $\mathbf{y}^{(m_r)}$. Where $\mathbf{y}^{(m_l)}$ contains all elements of $\mathbf{y}^{(m)}$ whose corresponding values of $\mathbf{x}^{(m)}_s \leq c$ and $\mathbf{y}^{(m_r)}$ all elements where $\mathbf{x}^{(m)}_s > c$. The gain (or reduction in error) from a split at node $m$ in predictor $\mathbf{x}_s$ at value $c$ is defined as:
\begin{equation}
  \label{eq:loss}
  \Delta(\mathbf{y}^{(m)}) = L(\mathbf{y}^{(m)}) - \left[\frac{n^{(m_l)}}{n^{(m)}} L(\mathbf{y}^{(m_l)}) +  \frac{n^{(m_r)}}{n^{(m)}} L(\mathbf{y}^{(m_r)})\right]
\end{equation}

Where $n^{(m_l)}$ and $n^{(m_r)}$ are the number of cases that fall to the right and to the left of the split, and $L(\cdot)$ is the loss function. 

In the example above we made the intuitive choice to use the number of cases incorrectly classified when assigning the mode as the fitted value, divided by the number of cases in the node, as the loss function. This proportion can also be interpreted as the impurity of the data in the node, to return to our goal stated at the beginning: to partition the data in a way that produces homogeneous subgroups. Therefore it is intuitive to use the amount of impurity as a measure of loss. This is how the algorithm can be used for outcomes with more than two unique values (i.e. for nominal or ordinal outcomes with more than two categories, or continuous outcomes). By choosing a loss function that is appropriate to measure the impurity of a variable at a certain level of measurement, the algorithm can be extended to those outcomes.

For categorical outcomes, denote the set of unique categories of $\mathbf{y}^{(m)}$ as $\mathcal{D}^{(m)} = \{y^{(m)}_i\}_{i\in\{1,\ldots,n^{(m)}\}}$. In order to asses the impurity of the node we first calculate the proportion of cases pertaining to each class $d \in \mathcal{D}^{(m)}$ and denote it as $p^{(m)}(d)$. Denote further the class that occurs most frequent as:

\begin{equation}
  \label{eq:maxclass}
  \def\argmax{\mathop{\rm argmax}}
  \hat{y}^{(m)} = \argmax_{d} p^{(m)}(d)
\end{equation}

Then the loss function can be applied to obtain the impurity of the node. The familiar misclassification loss is obtained from:

\begin{equation}
  \label{eq:misclass}
  L_d^{(m)}(\mathbf{y}^{(m)}) = \frac{1}{n^{(m)}} \sum_{i=1}^{n^{(m)}} \mathbb{I}(y^{(m)}_i \neq \hat{y}^{(m)}) = 1 - p^{(m)}(\hat{y}^{(m)})
\end{equation}

Where $\mathbb{I}(\cdot)$ is the indicator function that is equal to one when its argument is true. This formalizes the intuition used above: the impurity of the node is the proportion of cases that would be misclassified under "majority rule."[^asym_loss]

[^asym_loss]: The other two loss functions that are most often used are the Gini loss $L_{\text{gini}}(\mathbf{y}^{(m)}) = \sum_{d \in \mathcal{D}^{(m)}} p^{(m)}(d)[1-p^{(m)}(d)]$, and the entropy of the node $L_{\text{ent}}(\mathbf{y}^{(m)}) = -\sum_{d \in \mathcal{D}^{(m)}} p^{(m)}(d)\log[p^{(m)}(d)]$. Extensive theoretical [e.g. @raileanu2004theoretical] and empirical [e.g. @mingers1989empirical] work in the machine learning literature concluded that the choice between those measures does not have a significant impact on the results of the algorithm.

In the continuous case, the fitted value in a node is not calculated by majority vote. Typically the mean of the observations in that node is used as the predicted value for the observations in that node. To measure the impurity of the node usually the mean squared error (MSE) is used: $\hat{y}^{(m)} = \bar{y}^{(m)}$, where $\bar{y}^{(m)}$ is the mean of the observations in $\mathbf{y}^{(m)}$[^survival].

\begin{equation}
  \label{eq:mse}
  L_{\text{mse}}(\mathbf{y}^{(m)}) = \sum_{i=1}^{n^{(m)}} (y^{(m)}_i - \hat{y}^{(m)})^2
\end{equation}

[^survival]: This algorithm can also be applied to censored data. See @ishwaran2008random and @hothorn2006survival for details.

The extension to ordered discrete predictors is straightforward. Since the observed values of a continuous random variable are discrete, the partitioning algorithm described above works in the same way for ordered discrete random variables. Unorderd categorical variables are handled differently. If a split in category $c$ of an unordered discrete variable is considered, the categorization in values to the left and to the right of $c$ has no meaning since there is no ordering to make sense of "left" and "right." Therefore all possible combinations of the elements of $\mathcal{D}^{(m)}$ that could be chosen for a split are considered. This can lead to problems for variables with many categories. For an ordered discrete variable the number of splits that the algorithm has to consider is $|\mathcal{D}^{(m)}|-2$, however, for an unordered variable it is $2^{|\mathcal{D}^{(m)}|-1}-1$. This number gets large very quickly. For example the inclusion of a country indicator might be prohibitive if there are more than a handful of countries (e.g. if there are 21 countries in the sample the number of splits that have to be considered for that variable at each node is more than a million). Solutions to that problem are to include a binary variable for each category or to randomly draw a subset of categories at each node [see @louppe2014understanding, for details on the latter method].

After a loss function is chosen, the algorithm proceeds as described in our example. At each node $m$, $\Delta(\mathbf{y}^{(m)})$ is calculated for all variables and all possible splits in the variables. The variable-split combination that produces the highest $\Delta$ is selected and the process is repeated for the data in the resulting daughter nodes $\mathbf{y}^{(m_l)}$ and $\mathbf{y}^{(m_r)}$ until a stopping criterion is met. The stopping criterion avoids trees that are too complex and therefore over fit the data. Theoretically a tree could be grown until each observation has its own terminal node. This tree would do perfectly on the data it was fitted to but would perform very poorly on new data, because all the noise of the training data is included in the model. A stopping criterion is therefore a method to find a balance between a tree that is too complex and overfits the data and a tree that is too simple and therefore smoothes over important details. This mirrors the bias-variance tradeoff relevant to all statistical models [@hastie2009elements ;@farissjones2015]. Stopping criteria that are commonly used include the depth of the tree (how many levels of splits does the tree have), the number of observations in the terminal nodes or the homgeneity of the distributions in the terminal nodes. The right choice of a value for these criteria depends on the problem at hand and should be understood as a tuning parameters. That means, they should be chosen to minimize the expected generalization error for example by using cross validation.

Once the tree is completely grown, a predicted value for each observation is obtained, as in our example, by assigning a summary statistic for the terminal node the observation ended up in. For continuous data usually the mean of the distribution in the terminal node is used. For categorical data, either the majority category, or a vector of predicted probabilities for each category is assigned. Figure \ref{fig:cart_approx} illustrates how the predicted values from CART can approximate the function connecting the outcome and the predictor. 

\input{figures/latex_subfloats/approximation.tex}

After a tree has been "grown" on the data, predicted values for new data can be obtained in a straightforward manner. Starting at the first node of the tree, a new observation $i$ is "dropped down the tree", according to its values of the predictors $(x_{i1},...,x_{ip})$. That is, at each node, the observation is either dropped to the right or the left daughter node depending on its value on the predictor that was used to make a split at that node. This way, each new observation ends up in one terminal node. Then the predicted value of this terminal node is assigned as the prediction of the tree for observation $i$.

As previously mentioned CART has two main problems: fitted values have high variance and there is a substantial risk of overfitting. Fitted values can be unstable, producing different classifications when changes to the data used to fit the model are made (i.e., the estimator has high variance). There are several related reasons why this occurs. The first is that CART is locally optimal, that is, each split is optimal only at the node at which it occurs. Globally optimal partitioning is generally computationally intractable. Instead heuristic algorithms that are locally optimal (greedy) are used.[^global] Given this locally optimal optimizaton, order effects result, that is, the order in which the variables are split can result in different resulting tree structures, and thus, different predictions. Random Forests, which we discuss in the next section, have much lower variance and remove the effects of ordering.

[^global]: Though see @grubingerevtree for an example of a stochastic search algorithm for this problem.

## Random Forests

@breiman1996bagging proposed bootstrap aggregating, commonly called "bagging," to decrease the variance of fitted values from CART. This innovation also can be used to reduce the risk of overfitting. A set of bootstrap samples are drawn from the data: samples drawn with replacement and of the same size as the original data. A CART is fit to each of these samples. Each bootstrap sample excludes some portion of the data, which is commonly referred to as the out-of-bag (OOB) data. Each tree makes predictions for the OOB data by dropping it down the tree that was grown without that data. Thus each observation will have a prediction made by each tree where it was not in the bootstrap sample drawn for that tree. The predicted values for each observation are combined to produce an ensemble estimate which has a lower variance than would a prediction made by a single CART grown on the original data. For continuous outcomes the predictions made by each tree are averaged. For discrete outcomes the majority class is used. Relying on the OOB data for predictions also eliminates the risk of overfitting since the each tree's prediction is made with data not used for fitting.

@breiman2001random extended the logic of bagging to predictors, resulting in Random Forests. Instead of choosing from all predictors for the split at each node, only a random subset of the predictors are used: increasing the diversity of splits across trees, which allows weaker predictors to have an opportunity to influence the models' predictions. This results in a further decrease in the variance of the fitted values (beyond bagging observations) and allows the use of large numbers of potentially relevant predictors (many more predictors than observations in some cases). A particular observation can fall in the terminal nodes of many trees in the forest, each of which, potentially, can give a different prediction. Again the OOB data, that is, data that was *not* drawn in the bootstrap sample used to fit a particular tree, is used to make each tree's prediction. For continuous outcomes, the prediction of the forest is then the average of the predictions of each tree:

\begin{equation}
  \label{eq:rf}
  \hat{f}(\mathbf{X}) = \frac{1}{T} \sum_{t=1}^T f^{(t)}(\mathbf{X}_{i \in \bar{\mathcal{B}}^{(t)}})
\end{equation}

where $T$ is the total number of trees in the forest, and $f^{(t)}(\cdot)$ is the $t$'th tree, $\bar{\mathcal{B}}^{(t)}$ is the out-of-bag data for the $t$'th tree, that is, observations in $\mathbf{X}^{(t)}$ and not in $\mathcal{B}^{(t)}$, the bootstrap sample for the $t$'th tree. For discrete outcomes, the prediction is the majority prediction from all trees that have been grown without the respective observation. Figure \ref{fig:rf_approx} displays the approximation to a function relating a continuous outcome to a single predictor obtained from a random selection of 25 trees from a Random Forest. It can be observed that the approximation is much smoother compared to the approximation by a single tree (see \ref{fig:cart_approx}).

The number of candidate predictors available at each node is a tuning parameter and should be chosen to minimize expected generalization error. Random Forests compare favorably with other popular nonparametric methods in prediction tasks and can be interpreted substantively as well as we will show in the following sections [See e.g., @breiman2001random; @breiman2001statistical; @cutler2007random; @murphy2012machine; @hastie2009elements].


## Exploratory Data Analysis and Substantive interpretation 

As could be observed in the section on CART, a single tree is relatively easy to interpret. It can be visualized as in \ref{fig:cart_visu} and directly interpreted. But how to interpret a thousand trees, every single one only fit to a sample of the data, and using a random sample of explanatory variables on each split? Because the Random Forest receives its meaning only as an ensemble, it would be fruitless to try to extract substantive insight from its pieces. However, several methods have been developed to extract more information than just predictions from the Random Forest. In this section we explain these methods how to interpret them substantively using visualizations from our \texttt{R} software package. 

In order to illustrate the practical application of Random Forests for EDA, we use two data examples from recently published political science studies. The first is a turnout study on released prisoners [@gerber2014can]. This dataset contains information on the experimental treatments as well as four additional covariates, on over 5000 former inmates from Connecticut prisons that were released and whose voting rights have been restored. As the treatment @gerber2014can sent letters with two different treatments (the control group was not contacted), encouraging them to register and vote. Turnout and registration rates were recorded. The authors found that their treatment had positive effects on registration and turnout. Since registration and turnout rates are very low in this population and in order to avoid a very imbalanced outcome, we use the subsample of registered prisoners and use their turnout as the outcome. We chose this data set, to show that Random Forests are useful not only in classical data mining applications with large numbers of predictors, but also for analysis in more standard political science applications.

As the second data set we consider data from a recent study of cross-national patterns of state repression [@hill2014empirical]. Quantitative analysis of cross-national patterns of state repression relies on annual country reports from Amnesty International and the United States Department of State, which are used to code ordinal measures of state repression such as the Cingranelli and Richards Physical Integrity Index and the Political Terror Scale [@ciri2010; @woodgibney2010]. We use the measure from @fariss2014respect which is based on a dynamic measurement model, which aggregates information from multiple sources on state repression in each country-year into a continuous measure. @hill2014empirical use data from 1981 to 1999 in their original study, however, the Random Forest algorithm assumes independent data (see the section on limitations). Since the measures of state repression are highly correlated in time within countries, for the purpose of this demonstration we use data just on the year 1999. The data set contains data on [????] countries. We use a set of explanatory variables that differs slightly from that of @hill2014empirical, containing some predictors that may be relevant but were omitted, and some predictors, such as the participation competitiveness component of Polity IV, and binary indicators for civil war, which have conceptual overlap with respect for physical integrity rights and should be omitted on those grounds [@hill2014empirical; @hill2014democracy]. This data is well suited for exploratory data analysis using Random Forests because we have no expectation that the relationship between any particular predictor (that is not binary) will have a linear (or even smooth) relationship with our measure of respect for physical integrity rights, nor do we have expectations about the number or size of any interactions that may be present in the data. However, we would like to discover such relationships if they exist. Additionally, by studying the latent similarity of countries in the predictor space, we hope to notice features of countries close in this space which we do not have data on, and might be fruitful areas of future research. 

### Permutation Importance

The standard measure of variable importance for Random Forests is permutation importance. The marginal permutation importance shows the mean decrease in prediction error that results from randomly permuting an explanatory variable. If a particular column of $\mathbf{X}$, say $\mathbf{x}_j$, is unrelated to $\mathbf{y}$, then randomly permuting $\mathbf{x}_j$ within $\mathbf{X}$ should not meaningfully decrease the model's ability to predict $\mathbf{y}$[^rndsplit]. However, if $\mathbf{x}_j$ is strongly related to $\mathbf{y}$, then permuting its values will produce a systematic decrease in the model's ability to predict $\mathbf{y}$, and the stronger the relationship between $\mathbf{x}_j$ and $\mathbf{y}$, the larger this decrease. Averaging the amount of change in the fitted values from permuting $\mathbf{x}_j$ across all the trees in the forest gives the marginal permutation importance of a predictor[^marginal]. Formally, for classification, the importance of explanatory variable $\mathbf{x}_j$ in tree $t \in T$ is:

[^rndsplit]: There are other methods of measuring variable importance. @ishwaran2007variable for example proposed to use the distance of the first split on a variable from the root node of the tree as a measure of importance. This measure uses the fact that important variables are selected early in the partitioning algorithm (see also the section on interaction detection in this paper). However, permutation importance is the most widely used measure of variable importance, we therefore refer the reader to the relevant literature for more details on alternative methods. 

[^marginal]: This measure is not truly marginal since the importance of a variable within a particular tree is conditional on all previous splits in the tree. It is possible to conduct a conditional permutation test which permutes $\mathbf{x}_j$ with variables related to $\mathbf{x}_j$ "held constant," reducing the possibility that a variable is deemed important when it is actually spurious [@strobl2008conditional]. However, this procedure is prohibitively costly in terms of computational resources.

\begin{equation}
  \label{eq:imp}
  \text{VI}^{(t)}(\mathbf{x}_j) = \frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_i^{(t)})}{|\bar{\mathcal{B}}^{(t)}|} -
  \frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_{i \pi j}^{(t)})}{|\bar{\mathcal{B}}^{(t)}|}
\end{equation}

\begin{equation}
  \label{eq:imp_av}
  \text{VI}(\mathbf{x}_j) = \frac{1}{T} \sum_{t=1}^T \text{VI}^{(t)}(\mathbf{x}_j)
\end{equation}

where $\bar{\mathcal{B}}$ is the out-of-bag data for tree $t$, $\mathbf{x}_j$ is a particular predictor, $\hat{y}_i^{(t)}$ is the fitted value for observation $i$ in tree $t$, $\hat{y}_{i \pi j}^{(t)}$ is the predicted class for the $i$'th observation after permuting $\mathbf{x}_j$, and $|\bar{\mathcal{B}}^{(t)}|$ is the number of observations *not* in the bootstrap sample used for fitting tree $t$. In other words, the importance in a single tree is simply the difference between the predictive accuracy (measured on the out-of-bag data) before and after permuting $x_j$ (Equation \ref{eq:imp}). The importance of variable $\mathbf{x}_j$ in tree $t$ is averaged across all trees to give the permutation importance for the forest (Equation \ref{eq:imp_av}) [@breiman2001random; @strobl2008conditional]. For regression, the permutation is defined similarly, by the average increase in the mean squared error across trees that results from permuting $\mathbf{x}_j$. Our software provides a unified extractor for permutation importance calculated by Random Forests in \texttt{R} as well as plotting functionality.

\input{figures/latex_subfloats/permutation_importance.tex}

Figure \ref{fig:imp} displays the permutation importance for the two data examples as produced by \texttt{edarf}. Figure \ref{fig:latent_imp} shows the importance of the explanatory variables for respect for human rights. Since the latent outcome in this example is a continuous variable, the importance is measured in increase in mean squared error (MSE) from permuting the variable of interest. For example, randomly permuting the column related to Judicial Independence in the data set, dropping down this new data set all the trees, measuring the difference in predictive accuracy for each tree and averaging over these measures for the forest results in an increase in MSE of about 0.4. This measure can be used to assess the relative importance of variables relative to each other. 

[CAN YOU ADD SOMETHING ON THE SUBSTANTIVE INSIGHTS HERE, IS THERE ANYTHING SURPRISING?]

Figure \ref{fig:imp_cond_vote} displays the permutation importance for the prisoners example. The scale of this measure is the increase in misclassification error as described in Equation \ref{eq:imp}. The predictive accuracy of the model is generally very bad, because the variables available do not contain much information on the decision to vote. The only variable that significantly contributes to the predictive accuracy of the model is if the subject voted in the previous election. Permuting this variable decreases the accuracy of the classification in voters and non-voters by about 2\%. The other variables actually *decrease* our ability to predict the outcome. [SOME EXPLANATION ON THIS FACT WHAT DOES IT MEAN? IS IT JUST RANDOM?]

This short exposition showed that permutation importance can be a useful tool to get a rough assessment of the predictive importance of the variables in the model. However, the usefulness of this measure of variable importance depends on the goals of the researcher. Since it is a measure of predictive importance it does not tell us anything on the causal importance of a variable. In order to find such effects, causal identification has to be provided through the research design. However, discovery and testing of such effects is not the goal of EDA. For such an analysis, permutation importance is a useful tool to get a rough assessment of potentially theoretically interesting variables. Especially in cases where many predictors are available, but little theory on important variables, this importance measure provides the possibility to easily screen for important variables. 

[A BIT MORE ON WHY THIS IS BETTER THAN E.G. SIGNIFICANCE TESTS OR CORRELATIONS]

[SHOULD WE ADD SOMETHING ON THE EPISTEMOLOGICAL QUESTIONS RELATED TO PREDICTIVE PERFORMANCE AS MEASURE OF THEORETICAL IMPORTANCE?] 


### Partial Dependence

Although the rough importance of a variable can often be very insightful, most scholars are interested in how the variable is related to the outcome. Partial dependence is a simple method, again based on predictions from the forest, to visualize the partial relationship between the outcome and the predictors [p. 369, @hastie2009elements]. Partial dependence provides the possibility to visualize the relationship between $\mathbf{y}$ and one or more predictors $\mathbf{x_j}$ as detected by the Random Forest. The basic intuition is to obtain a prediction from the Random Forest for each value of $\mathbf{x_j}$ (or of each value combination if there are multiple variables of interest). Plotting these predictions against the unique values of $\mathbf{x_j}$ then displays how $\mathbf{y}$ is related to $\mathbf{x_j}$ according to the model. Since the Random Forest can approximate almost arbitrary functional relationships between $\mathbf{x_j}$ and $\mathbf{y}$ -- as shown in Figure \ref{fig:rf_approx} -- the model is able to detect non-linear relationships without the need to pre-specify them. This allows to detect potentially interesting non-linearities in settings where little a priori theoretical knowledge allows for pre-specification of specific forms. This is one of the main strengths of Random Forests for exploratory analyses.  

The basic way partial dependence works is the following: for each value of the variable of interest, a new data set is created, where all observations are assigned the same value of the variable of interest. Then this data set is dropped down the forest, and a prediction for each observation is obtained. By averaging over these predictions, an average prediction, for a synthetic data set where the variable of interest is fixed to a particular value, and all other predictors are left unchanged is obtained. This is similar in spirit to integrating over the parameters corresponding to $\mathbf{X}_{\bar{S}}$, however since we have no explicit probability model, we use this empirical procedure. Repeating this for all values of the variable of interest gives the relationship between said variable and the outcome over its range. In more detail, the partial dependence algorithm works as follows:

 1. Let $\mathbf{x}_j$ be the predictor of interest, $\mathbf{X}_{-j}$ be the other predictors, $\mathbf{y}$ be the outcome, and $\hat{f}(\mathbf{X})$ the fitted forest.
 2. For $\mathbf{x}_j$ sort the unique values $\mathcal{V} = \{\mathbf{x}_j\}_{i \in \{1, \ldots, n\}}$ resulting in $\mathcal{V}^*$, where $|\mathcal{V}^*|=K$. Create $K$ new matrices $\mathbf{X}^{(k)} = (\mathbf{x}_j = \mathcal{V}^*_k, \mathbf{X}_{-j}), \: \forall \, k = (1, \ldots, K)$.
 3. Drop each of the $K$ new datasets, $\mathbf{X}^{(k)}$ down the fitted forest 
 resulting in a predicted value for each observation in all $k$ datasets: $\hat{\mathbf{y}}^{(k)} = \hat{f}(\mathbf{X}^{(k)}), \: \forall \, k = (1, \ldots, K)$.
 4. Average the predictions in each of the $K$ datasets, $\hat{y}_k^* = \frac{1}{n}\sum_{i=1}^N \hat{y}_i^{(k)}, \: \forall \, k = (1, \ldots, K)$.
 5. Visualize the relationship by plotting $\mathbf{V}^*$ against $\hat{\mathbf{y}}^*$.

The average predictions obtained from this method are more than just marginal relationships [IS THIS CORRECT, CONNECT TO THE COMPARISON SECTION] between the outcome and the predictor. Since each of the predictions are made using all the information in all the other predictors of an observation, the prediction obtained from the partial dependence algorithm also contains this information. This means that the relationship displayed in a partial dependence plot contains all the relation between $x_j$ and $y$ including the averaged effects of all interactions of $x_j$ with all the other predictors $\mathbf{X}_{-j}$, which is why this method gives the partial dependence rather than the marginal dependence. [CHECK THIS WHOLE SECTION] Our software provides a method to compute $k$-way partial dependence (i.e., interactions of arbitrary dimension or many two-way partial dependencies) for continuous, binary, categorical, censored, and multivariate outcome variables.

\input{figures/latex_subfloats/partial_dependence.tex}

Figure \ref{fig:pd} diplays the partial dependence for our example data sets. Figure \ref{fig:latent_pd} displays the relationships of selected predictors with the latent outcome variable. Figure  \ref{fig:pd_cond_vote} shows the results for the prisoners example. Note that the partial dependence plots in \ref{fig:latent_pd} additionally contain error bars for the predictions, whereas there are no such bars in the plots for the prisoners example. The treatment of sampling uncertainty in predictions of machine learning algorithms is subject to current research in the relevant literature [CITATIONS]. @wager2014confidence developed a method -- the bias corrected infinitesimal jackknife (BIJ) to produce variance estimates for predictions from Random Forests. However, this method works only for continuous outcomes and not for classification. We implemented the BIJ in our software package to produce uncertainty intervals for partial dependence plots. Since this is a topic of ongoing research, and there are other proposed approaches, we expect to extend our software to these approaches [see e.g. CITATION].

The partial dependence is interpreted as the predicted value for a particular value of an explanatory variable averaged within the joint values of the other predictors. This is similar to the interpretation of outputs from other parametric, semi-parametric, or non-parametric regression methods when calculating marginal effects (below we give a more detailed account on how they compare). In the human rights example, the partial dependence reveals some interesting non-linearities in the relationship between the latent respect for human rights and the explanatory variables. For instance, the most important variable, according to permutation importance, judicial independence, seems to have it's biggest effect[^effect] going from the value two to three compared to from one to two [I HAVE NO CLUE ABOUT THE SUBSTANCE HERE< THIS MIGHT NEED REVISION]. The algorithm predicts on average a value of about 0.3 on the latent respect for human rights scale for countries that have a value of one on the judicial independence measure. For countries with high judicial independence, the algorithm predicts average respect of one (see @fariss2014respect for the interpretation of the latent scale). Recall that this is an average prediction, because the value of judicial independence is assigned to every observation in the data set, and the predictions from these 'counterfactual observations' are averaged. 

For the prisoners example, the y-axis of the partial dependence plots is the averaged predicted probability to vote given that the subject registered. For the time since release the algorithm discovered a u-shaped relationship. The plot for the ordinal treatment shows that ...[WHAT IS THE CODING OF THE TREATMENT].

[^effect]: Note, that the word effect does not imply a causal effect. The relationships discovered by the are just statistical dependencies in the data that have to be interpreted with the same care as partial correlation obtained, e. g. from regression models without causal identification.   


### Interaction Detection

Partial dependence can also be used to visualize interactions the algorithm may have found. One way to do this, is to create a dataset for each of the possible combinations of unique values of the explanatory variables that are involved in the interaction of interest, and calculate the partial dependence for each of these pairs, as described above. Figure \ref{fig:int_prison} displays this procedure to check if there is an interaction between the treatment and the years since release. The three panels correspond to the three experimental groups. An interaction is detected if the shape of the relationship of one variable with the outcome changes across levels of the other variable. In Figure \ref{fig:pd_prison} a shift in the level of the probability to vote can be observed. This shift is due to the marginal effect of the treatment. However, a change in the shape of the relationship is visible, too [SOMETHING IS WEIRD IN FIGURE WRITE MORE WHEN CLARIFIED].

\input{figures/latex_subfloats/interaction.tex}

In this case, one variable involved in the interaction has only three levels. However, if the variables involved in a potential interaction are both continuous or categorical with many levels, two problems occur. First, it would be computationally prohibitive to calculate the partial dependence for all combinations of values of the two variables. For instance, if analyzing the interaction between two variables with $50$ and $100$ unique values, $50 \times 100 = 5000$ data sets would have to be created and processed by the partial dependence algorithm. Second, the visualization becomes more difficult, because the there are two many value combinations. 

We solve the first problem by taking a random sample of unique values that are used in the algorithm[^extrapolation] (but always including the minimum and maximum of the variables involved to obtain a picture of the whole range of both variables). The reduction of the number of values that is achieved with this strategy also makes visualization easier. Another option for visualization, if reduction of unique values is not desirable, is to use three dimensional plots, as displayed in Figure \ref{fig:int_3d}.  [WOULDBE BETTER TO HAVE ONE WITH OUR EXAMPLE]

[^extrapolation]: It is also possible to use an evenly spaced grid, however, this may result in extrapolation. Both of these options are implemented in our \texttt{R} package.

Theoretically, higher order interactin could be detected in this way, too. If, for instance, a potential three-way interaction is considered, all combinations of the unique values of the three variable can be obtained and partial dependence can be calculated. In addition to the computational demand of such a procedure, the partial dependence itself is difficult to interpret and is almost exclusively useful when visualized. In practice, it is therefore hard to consider interactions of a higher order for substantive interpretation[^interaction]. In order to screen the set of explanatory variables for interactions, the partial dependence of all variable pairs has to be calculated and visually inspected. Depending on the number of predictor variables this procedure might be computationally and time intensive. 

[^interaction]: Although it is difficult to interpret these higher order interactions, they are still detected by the Random Forest algorithm and built into the model after it is fitted. The information from such higher order interactions is therefore still contained in predictions obtained from the forest.

Another method of detecting interactions from Random Forests does not rely so heavily on visualization. Instead, it is relying on the concepts of maximal $v$-subtrees and minimal depth [@ishwaran2007variable; @ishwaran2010high; @ishwaran2011random]. The basic idea behind this method for interaction detection is the fact that in CART variables that are used for splits on a higher level (i.e. closer to the root node) have higher importance for prediction[^vimp]. The measure of interactive importance of two variables, say $v$ and $w$ is the depth of the first split on $w$ within a tree that is defined by the first split on $v$. More formally, @ishwaran2010high introduce the concept of a maximal $v$-subtree. A maximal subtree for a variable $v$ is the largest subtree of a tree, that has as its root node a split on $v$, and no split on $v$ in the parent nodes of this root node. The minimal depth for variable $v$ is the distance between the highest maximal subtree for variable $v$ and the root node of the whole tree. The minimal depth is therefore a measure of the predictive importance of $v$. Interactions between pairs of variables $v$ and $w$ can be detected, by calculating the minimal depth of $w$ in the maximal subtree of $v$ and averaging this measure over all trees in the forest. This calculation gives a matrix of size $p \times p$ (recall that $p$ is the number of variables), that has on the diagonal the importance (according to the minimal depth criterion) of each variable, and on the off-diagonal elements the importance of the pairs of variables. This matrix can be easily scanned for rows that have a high importance on the diagonal entry and high importance on the off diagonals indicating interactions with these variables[^visu].

This method is easier to implement, and detection of interaction in situations with many explanatory variables is more feasible. However, it only works with pairs of variables. There are, to our knowledge, no ways to detect and interpret interactions of higher orders. However, the interactions are still contained in the fitted forest. This means, the information about these interactions are contained in all predictions and, when they exist, discovered by multivariate partial dependence plots.

[^vimp]: This fact can also be used to create a measure of variable importance; an alternative to the permutation importance measure described above.
[^visu]: We are currently working on the implementation of a visualization tool for this method across packages. Currently maximal subtrees and minimal depth can only be obtained from the \texttt{randomForestSRC} package.


### Similarity and Clustering

Random Forest can furthermore be used to to understand the similarity between observations in the predictor space, after the forest has been fit. Offering an easy tool for cluster analysis in exploratory settings. To obtain the similarity of observarions, a proximity matrix is calculated, which is an $n$ by $n$ matrix where each entry gives the proportion of times that observation $i$ is in the same terminal node as observation $j$ across all the terminal nodes in the different trees of the forest. Since observations that have similar $\mathbf{x}$ values 'travel' the same way on splits more often than values with dissimilar values, the co-occurence in terminal nodes is a suitable measure of similarity. 

In order to interpret this large proximity matrix, matrix factorization methods such as principle components analysis (PCA) can be used to visualize the similarity of the observations on at least two dimensions of the approximation to the proximity matrix. PCA can be effectively visualized using a biplot [@gabriel1971biplot]. Our software provides a unified interface for extracting, finding an approximation to, and visualizing proximity matrices. We also make it easy to layer additional variables on top of a biplot, e.g., colouring observations according to their level of some predictor as we show in Figure \ref{fig:prox}.

\input{figures/latex_subfloats/proximity.tex}

In Figure \ref{fig:prox_top} observations are coloured by their treatment condition, and in Figure \ref{fig:prox_bottom}, by whether or not they voted (the outcome of interest). The point shape shows whether or not the individual voted in 2008, and the size of the point gives the individual's age on election day. Clearly the first component (the $x$-axis) is whether or not the individual voted in 2008, and the second component is treatment status. Age appears to be less directly relevant, though it may be indirectly relevant. Similar figures for Random Forests predicting whether or not an individual votes regardless of whether they registered and for registration can be found in the appendix.

[MORE EXPLANATION HERE]

## Limitations

Random Forests are not without issue however. The CART which they are composed of often rely on biased splitting criteria: some types of variables, specifically variables with many unique values, are artificially preferred to variables with fewer categories [@hothorn2006unbiased; @strobl2007bias]. These biases can also affect the measures of variable importance. Recent developments have resulted in unbiased recursive partitioning algorithms that separate the variable selection and split selection parts of the CART algorithm, and utilize subsampling rather than bootstrapping [@hothorn2006unbiased]. The analyses in this paper are done using this unbiased algorithm. 

Furthermore, as other more standard methods, Random Forests are based on the assumption that the observations in the sample are independent. However, unlike many well developed parametric methods, there are not many well implemented and studied solutions for such data. Not modeling such structures like spacial or temporal dependence can have several negative consequences. It may decrease predictive performance on new data and mislead us about the importance of variables strongly related to different features of the unmodeled dependence structure [WHAT? MORE EXPLANATION HERE]. 

However, there are several ways this dependence can be modeled, and much research is currently done to improve the performance of Random Forests on such data. One way is to include variables indicating structure in the data as explanatory variables. Then, at each node in each tree in the forest, these variables have a chance of being included in the set of variables that may be split on. This means that if an explanatory variable has a relationship with the outcome that changes across different units, time, etc., this can be detected if the structure is included in this way. However, when such a description of the data structure results in high dimensional unordered categorical variables, this is computationally intensive and not always possible. As mentioned in the section on CART, inclusion of a countty indicator might already be prohibitive if there are more than 20 countries. 

It is also possible to replace the discrete uniform bootstrap or subsampling used by Random Forests with a nonparametric bootstrap for dependent data, such as the generalized moving block bootstrap, transformation based bootstraps, or model-based (filtering) bootstrapping [@lahiri2003resampling]. [WHAT DOES THIS DO, HOW DOES THIS HELP WITH THE PROBLEM?]

Alternatively, a random effects approach could be used: the outcome of interest is treated as a function of an unknown regression function, which is estimated using Random Forests and completely pools the data, and set of unit random effects for which we estimate the variance, and idiosyncratic error which is assumed uncorrelated with the random effects [@hajjem2014mixed; @hajjem2011mixed] [WHAT?]. Yet another alternative approache is to sample units -- for example countries -- or sampling units and then an observation from within that unit [@adler2011ensemble] [ALSO NOT CLEAR]. Furthermore methods known from classical time serie, the analyst can transform the dependent variable, such as by subtracting the within-unit mean. This, however, invalidates the use of the fitted Random Forest for the prediction of new data.

## Conclusion

In situations where relevant theory says little about the functional form of the relationship of interest, the magnitude and degree of nonlinearity and interaction is unknown, the number of possibly relevant predictors is large, or when prediction is important, Random Forests may be quite useful. We believe that this situation is common, especially in comparative politics and international relations.

We have provided a technical introduction to CART and Random Forests, with the goal of providing researchers with enough background knowledge to use these methods in an informed manner, as well as software to reduce the technical burden of their use. Random Forests are but one member of a broad class of supervised machine learning methods that could be useful to political scientists. We hope that in the future these methods and the issues that they are designed to solve are incorporated into political science. Particularly, we see the development of machine learning methods for dependent data as a fruitful area for future research.

# References
