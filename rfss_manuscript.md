% Random Forests for the Social Sciences
% Zachary Jones and Fridolin Linder[^contact]

[^contact]: Zachary M. Jones is a Ph.D. student in political science at Pennsylvania State University ([zmj@zmjones.com](mailto:zmj@zmjones.com)). Fridolin Linder is a Ph.D. student in political science at Pennsylvania State University ([fridolin.linder@gmail.com](mailto:fridolin.linder@gmail.com)).

\begin{abstract}
We introduce random forests, a nonparametric supervised learning algorithm that allows researchers to nonparametrically the relationship between an outcome that is continuous, categorical, or censored (survival). Random forests detect interaction and nonlinearity without prespecification, have low generalization error, do not overfit, and can be used with many correlated predictors. Importantly, they can be interpreted in a substantively relevant way via measures of marginal variable importance and the partial dependence algorithm. We provide intuition as well as technical detail about how random forests work, in theory and in practice, as well as an empirical example from the literature on comparative politics. We also provide to facilitate the substantive interpretation of random forests and guidance on when random forests may be useful.
\end{abstract}

## Introduction

Political scientists have begun to utilize more flexible algorithmic methods for inferring substantive relationships from data [@beck1998beyond; @beck2000improving; @hainmueller2013kernel; @hill2014empirical]. These methods can often outpeform more commonly used regression methods at predicting data not used to fit the model, which is useful for policymakers and serves as a useful check of the explanatory power of our theories [@hill2014empirical]. Many of these methods are commonly thought of as "black box," that is, they predict well, but do not permit substantive interpretation [@breiman2001statistical]. We show that this is *not* the case with a broadly applicable, powerful method: random forests [@breiman2001random]. Random forests are especially useful to political scientists because of their ability to approximate arbitrary functional forms, be used with continuous, discrete, and censored (survival) outcomes, and because they permit substantive interpretation via permutation importance measures and the partial dependence algorithm. We provide an introduction to the theory and use of random forests, a substantive example drawn from the literature on comparative politics, and provide software to make substantive inference based on random forests easy.

We think that random forests would be useful in political science when relevant theory says little about the functional form of the relationship of interest, when the magnitude and degree of nonlinearity and interaction is unknown, when the number of possibly relevant predictors is large, and when prediction is important. Random forests can approximate many nonmonotone, nonlinear functional forms. Interactions and nonlinearities are identified by random forests without prespecification, which decreases prediction error and allows researchers to study the relationships discovered by the algorithm. Random forests allow the inclusion of more predictors than observations. Though this situation is not common in political science (though see the literature on behavioral genetics, wherein this problem does occur), related issues such as a highly correlated predictors are not an issue for random forests. Prediction is important for theory evaluation and for policymakers, and random forests' predictive performance relative to common parametric regression methods and other nonparametric methods is strong.

Random forests are composed of decision trees. A decision tree is created by recursive partitioning, which is a method for finding homogenous (in the outcome variable) subsets of the data using the predictors [@breiman1984classification]. Different definitions of homogeneity allow decision trees to be used with different types of outcomes. The fitted values of decision trees have low bias but high variance. The variance of fitted values can be reduced by growing multiple decision trees with bootstrap samples of the data, and then averaging over the predictions made by each tree: a procedure known as "bagging" [@breiman1996bagging]. Random forests are composed of decision trees, but, as we discuss in the next section, random forests are more than just an ensemble of decision trees. The key innovation of random forests: random selection of a subset of the predictors at each  possible split in each constituent decision tree, results in predictions with less bias and variance [@breiman2001random]. We treat these methods in greater detail in the next section. This is followed by a discussion of methods for extracting substantively relevant information from random forests, and an empirical example.

## Classification and Regression Trees

The description of the CART algorithm that follows is based on work by @breiman1984classification. More complete overviews of CART and its history can be found in @berk2008statistical, @murphy2012machine, and @hastie2009elements, among others.

Classification and regression trees (CART) are a regression method that relies on repeated partitioning of the data to estimate the conditional distribution of a response given a set of predictors: $y|\textbf{X}$. We first provide a high-level description of the algorithm and then describe it in greater detail in the next paragraph. The algorithm works by examining every unique value in the set of predictors (a candidate for a binary split), and calculating the homogeneity of the distribution of the outcome variable that would result by grouping observations that fall on either side of the value considered. The unique value across all of the predictors considered that maximizes the degree to which the homogeneity in the groups of data that fall on either side of the split is chosen, resulting in two partitions of the data. This process repeats on each of the partitions until a stopping criteria is met. A prediction is made for each observation by summarizing the distribution of the outcome variable in the last partitions made (the terminal nodes), usually by taking the mean (for regression) or the mode (for classification).[^alternatives]

[^alternatives]: As is the case with more common forms of regression, it is not necessary to model the conditional mean. We could easily instead choose another summary of the conditional distribution of $y$ given $\textbf{X}$.

As previously noted, CART is a recursive partitioning algorithm. At the topmost node the algorithm searches through all available predictors and possible splits within each predictor. The optimal predictor-split combination is chosen using a cost function. Consider an explanatory variable $X_1$ and an outcome $y$ which is continuous. The unique values in $X_1$ are sorted, and for each unique value $c$ in $X_1$, the mean square error (MSE) is calculated for data that fall on either side of $c \in X_1$. If we denote $\mathcal{D}$ as the data in the current node, and $\mathcal{D}_L$ and $\mathcal{D}_R$ the data that fall on either side of a proposed split $c \in X_1$, the MSE is calculated for $\mathcal{D}_L$ and $\mathcal{D}_R$, summed, and then compared to the baseline MSE (the MSE if no split were selected) in $\mathcal{D}$. We refer to this as the regression cost: $\text{cost}(\mathcal{D}) = \sum_{i \in \mathcal{D}} (y_i - \bar{y})^2$. The gain in a split is defined as:

$$\Delta = \text{cost}(\mathcal{D}) - \left( \frac{|\mathcal{D}_L|}{|\mathcal{D}|} \text{cost}(\mathcal{D}_L) + \frac{|\mathcal{D}_R|}{|\mathcal{D}|} \text{cost}(\mathcal{D}_R)\right)$$

If a split is selected then this process is repeated for $\mathcal{D}_L$ and $\mathcal{D}_R$ until a stopping criteria is met. The stopping criteria may be that the tree has reached sufficient depth, that the number of observations that fall into $\mathcal{D}_L$ or $\mathcal{D}_R$ is too small, or the distribution of $y$ in $\mathcal{D}$ is sufficiently homogeneous (as measured by MSE or by another criteria).

If the outcome variable is discrete the classification cost can be quite different. A number of cost functions are available, such as the misclassification rate, entropy, deviance, or Gini index. In a particular node $\mathcal{D}$ we can define the class-conditional proportions as $\hat{\pi}_c = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} \mathbb{I}(y_i = c)$, where $c$ indicates the class. To use this to evaluate a particular split in a categorical predictor we first calculate the predicted class $\hat{y}_c = \operatorname*{arg\,max}_c \hat{\pi}_c$ and then apply a cost function such as the misclassification cost: $\frac{1}{|\mathcal{D}|} \sum_{i \in \mathcal{D}} \mathbb{I}(y_i \neq \hat{y}) = 1 - \hat{\pi}_y$, or the Gini index: $\text{cost}(\mathcal{D}) = 1 - \sum_c \hat{\pi}_c^2$.

With survival data, a log-rank statistic is often used. Let $T$ be the time of an event and $C$ the censoring time, then the data consist of $N$ observations $(y_i, \delta_i, \textbf{x}_i)$ where $y_i=\text{min}(T,C)$ and $\delta_i=\mathbb{I}(T \geq C)$. A  given node $h$ in a tree contains $M$ observations with survival times $t_1 < t_2 < \ldots < t_m$. A proposed split results in two daughter nodes $(j=1,2)$ containing a certain amount of cases. Let the $d_{i,j}$ denote the number of deaths and $y_{i \; j}$ the number of observations at risk at time $t_i$ in daughter node $j$ respectively. Then the survival difference resulting from a split at $c$ in predictor $X$ is measured by the log-rank score:

$$
L(x,c) = \frac{\sum_{i=1}^M d_{i1} - y_{i1}\frac{d_i}{y_i}}{\sqrt{\sum_{i=1}^M \frac{y_{i1}}{y_i}(1-\frac{y_{i1}}{y_i})(\frac{y_i-d_i}{y_i-1})d_i}}
$$

At each terminal node the cumulative hazard function (CHF), denoted $H$, is estimated. Each observation in terminal node $h$ is assigned the same CHF, where:

$$\widehat{H}(t|h) = \sum_{t_{lh} \leq t} \frac{d_{lh}}{y_{lh}}$$ 

CART has two main problems: fitted values have high variance, and there is a substantial risk of overfitting. Fitted values, the value of the response predicted by the algorithm, can be unstable, producing different classifications when changes to the data used to fit the model are made. Bootstrapped aggregating, or "bagging," was developed by @breiman1984classification to deal with these issues. Bagging works by drawing a large number of bootstrap samples, i.e., random samples with replacement and of the same size as the original data, fitting the model to each bootstrap sample, and then combining the fitted values across the bootstrap samples by computing summary statistics as appropriate, e.g., means for regression and modes for classification. To reduce the risk of overfitting the out-of-bag (OOB) error can be computed. Instead of using, for each tree, the observations *in* the sample used to fit the tree to make predictions, instead calculate fitted values using observations *not* in the sample used for fitting. Then summarize the fitted values across bootstrap samples as before. This results in summaries of prediction error that are less likely to overstate the accuracy of the model.

Another issue is the inclusion of categorical variables with many values. With structured data such as is common in political science, it may be interesting to ask how much predictive power is lost by permuting unit labels, and how different predictors interact with unit labels. As may be clear from the splitting proccess described above, the combinatorics of an exhaustive search over possible splits in a categorical variable quickly becomes computationally prohibitive. A categorical variable with $n$ categories can be split in $2^{n-1}-1$ ways. An alternative would be to create a binary variable for each category, however, it is beyond the scope of this paper to analyze the behavior of the summarization of block permuting a set of binary variables. It is likely that different strategies for modeling unit specific variation must be developed in this situation.

Lastly, there is the issue of bias in the splitting procedure, meaning that some CART algorithms tend to select some sorts of variables more than they should. Variables with many categories and large amounts of missingness have been shown to generate this bias, which results in biased permutation importance measures [@hothorn2006unbiased; @strobl2007bias]. The key difference in the splitting procedure is the separation of variable selection (in which variable the optimal split will be found) and the split search procedure. For substantive work we recommend that researchers use splitting criteria with good frequentist properties, though this must be counterbalanced with computational limitations.

## Random Forests

@breiman2001random extended the logic of bagging to predictors, resulting in random forests. Instead of each tree relying on a bootstrapped sample of observations on all variables, only a random subset of the predictors are used for each splitting node in a tree: increasing the diversity of splits across trees: allowing weaker predictors to have an opportunity to influence the models' predictions. This results in a further decrease in the variance of the fitted values (beyond bagging observations) and allows the use of large numbers of potentially relevant predictors (many more predictors than observations in some cases). A particular observation can fall in the terminal nodes of many trees in the forest, each of which, potentially, can give a different prediction. Typically the OOB data, that is, data that was *not* drawn in the bootstrap sample used to fit a particular tree, is used to make each tree's prediction. This results in a weighted average:

$$f(\textbf{X})= \frac{1}{M} \sum_{m=1}^M f_m(\textbf{X}_{\text{oob}})$$

where $M$ is the total number of trees in the forest, and $f_m(\cdot)$ is the $m$'th tree, and $\textbf{X}_m$ is the OOB data for that tree.

With survival data the averaging procedure is not as simple. For the OOB ensemble estimate $\widehat{H}^*$ of the CHF of an observation $i$, $\widehat{H}$ is averaged over all trees that where grown without the observation. The prediction error rate is calculates using Harrel's concordance index. The basic intuition behind this index is to check for each pair of observations if the predicted risk is higher for the observation that failed first. The predicted risk for observation $i$ with covariate vector $x_i$ is obtained from:

$$R = \sum_{k=1}^n \widehat{H}^*(t_k|\textbf{X}_i)$$

If this is the case, the prediction counts as correctly classified.

### Substantive Interpretation

Since, with random forests both predictors and observations are being sampled, no particular tree will give great insight into the model's overall prediction for each observation. There are, however, several ways to extract inferences from random forests. The most simple is partial dependence [@hastie2009elements]. The partial dependence algorithm works as follows:

 1. Let $X_i$ be the predictor of interest, $\textbf{X}_{-i}$ be the other predictors, $y$ be the outcome, and $f(\textbf{X})$ the fitted forest.
 2. If $X_i$ has $k$ unique values $V = (v_1,v_2,...,v_k)$, create $k$ new datasets, where $X_i = v_k$ for $\forall \: v$ and all $\textbf{X}_{-i}$ are the same as in the original dataset.
 3. Drop each of the new datasets, $X_i^{v_k}, \textbf{X}_{-i}$ down the fitted forest, resulting
 in a predicted value for each observation in all $k$ datasets, $\hat{y}_{X_i=v}$.
 4. Summarize the predictions in each of the $k$ datasets (for example by taking the mean).
 5. Visualize the relationship by plotting $V$ against $\hat{y}_{X_i=v}$.

With slight modification, this method can also be used to visualize any joint relationships (i.e. interactions) the algorithm may have found. To do this create a dataset for each of the possible combinations of unique values of the explanatory variables of interest, predict the outcome in each of these observations, and then find the mean or modal prediction for each of these unique value combinations. For computational reasons we do not always use every unique value when an explanatory variable takes on more an arbitrary number of unique values. In this paper we use a random sample of 24 unique values that $X_i$ takes on.[^extrapolation] This logic can be generalized to joint relationships of an arbitrary dimension, but we limit ourselves here to pairwise partial dependence. The interpretation of partial dependence: the average predicted value for a particular value of an explanatory variable averaged within the joint values of the other predictors.

[^extrapolation]: It is also possible to use an evenly spaced grid, however, this may result in extrapolation. Both of these options are implemented in our R package [edarf](https://github.com/zmjones/edarf/).

Another approach to extracting inferences from random forests relies on permutation tests for variable importance. Rather than attempting to characterize the partial dependence of one or more predictors on the response, the goal is instead to describe how the model's ability to predict $y$ depends on a particular predictor. If a particular column of $\textbf{X}$, say $X_i$, is unrelated to $y$, then randomly permuting $X_i$ within $\textbf{X}$ should not meaningfully decrease the model's ability to predict $y$. However, if $X_i$ is strongly related to $y$, then permuting its values will produce a systematic decrease in the model's ability to predict $y$, and the stronger the relationship between $X_i$ and $y$, the larger this decrease. Averaging the amount of change in the fitted values from permuting $X_i$ across all the trees in the forest gives the marginal permutation importance of a predictor.[^marginal] Formally, for classification, the importance of explanatory variable $X_i$ in tree $t$ is:

$$VI^{(t)}(X_i) = \frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_i^{(t)})}{|\bar{\mathcal{B}}^{(t)}|} -
\frac{\sum_{i \in \bar{\mathcal{B}}^{(t)}} \mathbb{I}(y_i = \hat{y}_{i \pi j}^{(t)})}{|\bar{\mathcal{B}}^{(t)}|}
$$

where $\bar{\mathcal{B}}$ is the out-of-bag data for tree $t$, $X_i$ is a particular explanatory variable, $\hat{y}_i^{(t)}$ is the fitted value for observation $i$ in tree $t$, $\hat{y}_{i \pi j}^{(t)}$ is the predicted class after permuting $X_i$, and $|\bar{\mathcal{B}}^{(t)}|$ is the number of observations in the sample used for fitting tree $t$. The importance of variable $X_i$ in tree $t$ is averaged across all trees to give the permutation importance for the forest [@breiman2001random; @strobl2008conditional]. This can be thought of as testing the null hypothesis of independence between $X_i$ and $y$ as well as all other explanatory variables $\textbf{X}_{-i}$. That is, $\mathbb{P}(y, X, \textbf{X}_{-i}) = \mathbb{P}(X_i)\mathbb{P}(y, \textbf{X}_{-i})$. For regression, the permutation is defined similarly, by the average increase in the mean squared error across trees that results from permuting $X_i$.

[^marginal]: This measure is not truly marginal since the importance of a variable within a particular tree is conditional on all previous splits in the tree. It is possible to conduct a conditional permutation test which permutes $X_i$ with variables related to $X_i$ "held constant," reducing the possibility that a variable is deemed important when it is actually spurious [@strobl2008conditional]. However, this procedure is prohibitively costly in terms of computational resources.

### Dependent Data

As previously mentioned, these methods are not designed for use with dependent data, such as is common in political science. Not modeling the dependence structure may decrease predictive performance on new data and mislead us about the importance of variables strongly related to different features of the unmodeled dependence structure. In its basic implementation the estimated regression function is the result of complete pooling of the data. There are several ways this dependence can be modeled. One way is to include a categorical variable with unit indicators as an explanatory variable. Then, this explanatory variable has a chance of being included in the set of variables that the algorithm may select for splitting at a particular node. This is computationally intensive and not always possible. Alternatively, a random effects approach could be used: the outcome of interest is treated as a function of an unknown regression function, which is estimated using random forests and completely pools the data, and set of unit random effects for which we estimate the variance, and idiosyncratic error which is assumed uncorrelated with the random effects [@hajjem2014mixed; @hajjem2011mixed]. Alternative approaches include sampling units [@adler2011ensemble] or sampling units and then an observation from within that unit [@adler2011ensemble]. Alternatively, the analyst can transform the dependent variable, such as by subtracting the within-unit mean. This, however, invalidates the use of the fitted random forest for forecasting. In the following applications we discuss what approach we use and leave the development of other approaches to future work.

### Missingness

Missing values are a perennial problem in real world data. Often missingness is ignored, which, at best, decreases the precision of estimates, and at worst biases them. Multiple imputation is frequently used to provide estimates of the values of missing values: this relies on the assumption that, conditional on the observed covariates, missingness is random [@rubin2004multiple; @honaker2010missing]. When analysis is exploratory or predictive, missingness has a different epistemological status and can be *useful* [@shmueli2010explain]. When the goal is prediction alone, informative missingness improves our ability to predict the outcome, and when doing exploration, understanding how missingness is informative is useful for future work. When predictors with missingness are categorical missingness can simply be recoded as an additional category. With continuous variables this is not possible. Additionally, this is not always optimal however. When using permutation importance to measure the degree to which a model's fit is degraded by permuting said variable recoding in this way conflates missingness that is informative with how informative the observed values are. An alternative would be to create a set of variables that indicate whether missingness in a particular predictor (or set of predictors) is present. This does not fix the problem of partially observed predictors, but this can be handled using multiple imputation or other ways to handle missing values such as surrogate splitting, which, since unbiased estimation of causal effects is not the goal, do not have to satisfy the stringent conditionally missing at random assumption previously mentioned. Frequently data are imputed once, and the data are treated as completely observed. This does however, result in a tendency to underestimate generalization error.

## State Repression

![The marginal permutation importance of explanatory variables estimated using random forests, using the dynamic latent measure of state repression developed by @fariss2014. The bar shows the mean decrease in mean squared error that results from randomly permuting the independent variable indicated. If the variable is important, permuting its values should systematically *decrease* predictive performance, whereas an unimportant variable should produce no decrease, or a random decrease in mean squared error. The error bars result the $.025$ and $.975$ quantiles of the distribution of this statistic from the repeated draws from the posterior of the latent variable estimated by @fariss2014. \label{fig:hr-imp}](figures/hr_imp.png)

![The partial dependence of the response of year, percentage of the population that is male and aged 15-25, executive constraint and participation competitiveness (from Polity), the natural logarithm of population, a count of the number of INGOs of which a government’s citizens are members measured by @hafner2005human (labeled INGOs), and contract intensive economy (CIE), a measure of life insurance contracting used by @mousseau2008contracting (from @beck2003economic). On the $y$-axis is the dynamic latent measure of repression developed by @fariss2014 and on the $x$-axis the predictor indicated by the above label. Note that the scale of $y$ differs for each sub-plot. The relationship between each predictor and the response is averaged within the joint values of the other predictors conditional on the relationships discovered by the random forest. All values of the explanatory variable for which there are predictions are observed or imputed values. \label{fig:hr-pd}](figures/hr_pd.png)

![The root mean squared error (RMSE) computed using for the training set (1981-1992) and the test set (1993-1999). The RMSE is calculated for each model fit to a random draw (100 in total) from the posterior of the measure of respect for physical integrity due to @fariss2014, and the error bars show the lower .025 and upper .975 quantiles. The point shows the median. The models shown are Least Angle Regression (LARS) [@efron2004least], a basis-expanded Generalized Additive Model regularized via the adaptive least absolute shrinkage and selection operator (LASSO) [@kenkel2013bootstrapped; @tibshirani1996regression], and a random forest [@breiman2001random].
\label{fig:hr-pred}](figures/hr_pred.png)

There is a large literature in political science on the determinants of state repression, especially violations of physical integrity such as killings, disappearances, torture, and political imprisonment. Quantitative analysis of cross-national patterns of state repression relies on annual country reports from Amnesty International and the United States Department of State, which are used to code ordinal measures of state repression such as the Cingranelli and Richards Physical Integrity Index and the Political Terror Scale [@ciri2010; @woodgibney2010]. We use the measure from @fariss2014 which is based on a dynamic measurement model, which aggregates information from multiple sources on state repression in each country-year into a continuous measure.

The literature on state repression has, like most areas of political science, been dominated by data modeling. A particular stochastic model for the data is assumed, and the parameters of the model are estimated using the data on hand. Then statistical inference about the model's parameters is used to support claims about the generality of the results (implicitly about causality or population quantities). It seems unlikely that the assumed model of the data generating process is correctly specified or even a reasonable approximation given the complexity of the dynamics of dissent and repression well known from micro-level quantitative and qualitative research, leaving open the question of what can be learned from estimates of parameters that govern such models absent strong predictive validity. Researchers sell the predictive power of their theories short by making strict assumptions about functional form that are not a part of their theory.

We instead aim to explore the available data, summarizing the relationships between the an expanded set of variables identified in @hill2014empirical as important explanations for state repression in the extant literature and the measure of respect for physical integrity rights from @fariss2014.[^units] We do this using random forests. We present the marginal permutation importance of each variable and the partial dependence of the estimated function on the values of a subset of the explanatory variables as well as a subset of the large set of possible two-dimensional joint dependencies discovered by the model.

[^units]: As discussed in the last paragraph of the sub-section title "[Classification and Regression Trees]" it is not possible to include a categorical variable of country labels in this example. There are 189 countries in the data for this example, resulting in $2^{189-1}-1$ possible splits.

To reiterate, the marginal permutation importance shows the mean decrease in prediction error that results from randomly permuting the independent variable. If the variable is an important predictor, permuting its values will result in a systematic increase in prediction error, while an unimportant predictor will result in no decrease or a random decrease in prediction error. Figure \ref{fig:hr-imp} shows the permutation importance of predictors of state repression taken from @hill2014empirical. The rank ordering of the predictor importance is somewhat different from that of the original study, though the set of data used here is somewhat different.

Participation competitiveness, a component of Polity that measures "the extent to
which alternative preferences for policy and leadership can be pursued in the political arena," is the most important predictor according to this measure [@marshallaggers2009]. That is, the prediction error increases a substantial amount (30\%) when it is randomly shuffled. This should not be surprising, since, as noted in @hill2013concept and @hill2014empirical, there is conceptual overlap between this measure and measures of state repression. The natural logarithm of population and GDP per capita, and civil war are unsurprisingly important as well. More interesting are the importance of measures of contract intensive economy (CIE, used by @mousseau2008contracting), rule of law (from the International Country Risk Guide), judicial independence (from @ciri2010), and youth bulges (measured by @urdal2006 and used by @nordas2013), which represent relatively understudied concepts that may be worth further investigation.

The partial dependence plots shown in Figure \ref{fig:hr-pd} indicates that the model discovers a variety of nonlinearity which may be missed by analysts relying on more restrictive data models. These plots show the predicted values of repeated draws of the latent measure of state repression averaged within the values of the other variables included. However it is possible that this "nonlinearity" is due to interactions with other included or excluded explanatory variables, or reflect a pattern due to unmodeled dependence in the data. Since this is EDA this can be investigated, for example by looking at the average fitted value within values of multiple predictors. A plot showing partial dependence for all of the predictors is available in the [Appendix]. The random forest has clearly discovered patterns in the data that do not comport with the assumptions commonly made about the function mapping the predictors to the response in commonly used data models. We suggest that a great deal more value could be extracted from observational data already collected but analyzed with methods that impose structure on the data that is inappropriate. Additionally, the quantities extracted from the model pertain directly to prediction (in the case of permutation importance), or are the result of a fitting process that is primarily concerned with prediction (in the case of partial dependence), which we think makes their application to observational data in political science quite natural.

Lastly we examine how well the model does at predicting latent respect for physical integrity rights. We split the data into two sets: a training set that covers 1981-1992 and a test set that covers 1993-1999. We repeatedly draw from the latent outcome variable's distribution, fit a model to the training set, predict the outcome in the test set, calculate the root mean square error,[^rmse] and then summarize the distribution of these error statistics across a large number of draws from the latent outcome variable using the lower .025 and upper .975 quantiles and the median. Absent intuitive scale for the outcome variable it is hard to judge the model's performance in absolute terms, thus we provide comparisons to other models. One thing to note is that there is an increase in the prediction error over time within the training and test data suggesting that there there is over-time variation in respect for physical integrity rights that we are not able to explain with the included measures. We hope that this provides a useful predictive benchmark for this measure of physical integrity rights.

[^rmse]: The RMSE is $\sqrt{\frac{1}{n}\sum_{i=1}^n (y - \hat{y})^2}$.

## Conclusion

# References

# Appendix

![Two-way partial dependence for all predictors included in the model of latent respect for physical integrity rights. The interpretation is the same as Figure \ref{fig:hr-pd}. Note that imputation of missing values results in some categorical/binary variables taking on non-integer values.](figures/hr_pd_all.png)
